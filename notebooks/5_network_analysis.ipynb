{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/conda/envs/datasci3/lib/python3.5/site-packages/PIL/Image.py:85: RuntimeWarning: The _imaging extension was built for another  version of Pillow or PIL\n",
      "  warnings.warn(str(v), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import random, pickle\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tweet data and mentions data to use in network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = spark.read.parquet('tweets_all.parquet')\n",
    "tweets.registerTempTable('tweets')\n",
    "\n",
    "mentions = spark.read.parquet('mentions_all.parquet')\n",
    "mentions.registerTempTable('mentions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5457"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~5476 tweets seem to be retweets but aren't explicitly marked as such. It's not worth trying to integrate these.\n",
    "\n",
    "temp = spark.sql(\"\"\"\n",
    "    select screen_name, text\n",
    "    from tweets\n",
    "    where text rlike '([^a-zA-Z0-9]|^)RT[^a-zA-Z0-9]+' and retweet_id is null\n",
    "\"\"\")\n",
    "temp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis\n",
    "\n",
    "### 1. Create edges based on retweets, quotes, mentions, and replies\n",
    "### 2. Sum those interactions for each dyad to produce weights\n",
    "### 3. Create dictionary of other user data for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create edge from users to people they retweeted\n",
    "# Weight edges by n_links\n",
    "tweet_rt_edge = spark.sql(\"\"\"\n",
    "    select t.screen_name as screen_name1, rt.screen_name as screen_name2, count(*) as n_links\n",
    "    from tweets as t\n",
    "    left join tweets as rt\n",
    "    on t.retweet_id = rt.tweet_id\n",
    "    where t.retweet_id is not null and rt.screen_name is not null\n",
    "    group by t.screen_name, rt.screen_name\n",
    "\"\"\")\n",
    "tweet_rt_edge.registerTempTable('tweet_rt_edge')\n",
    "tweet_rt_edge.persist(StorageLevel.MEMORY_AND_DISK);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679842"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_rt_edge.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[screen_name1: string, screen_name2: string, n_links: bigint, desc: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create edge from users to people they mentioned/quoted/replied to\n",
    "interaction_edge = spark.sql(\"\"\"\n",
    "        select screen_name as screen_name1, quoted_screen_name as screen_name2, count(*) as n_links, 'quote' as desc\n",
    "        from tweets as t\n",
    "        where quoted_screen_name is not null\n",
    "        group by screen_name, quoted_screen_name\n",
    "\n",
    "        union all\n",
    "\n",
    "        select screen_name as screen_name1, in_reply_to_screen_name as screen_name2, count(*) as n_links, 'reply' as desc\n",
    "        from tweets as t\n",
    "        where in_reply_to_tweet_id is not null\n",
    "        group by screen_name, in_reply_to_screen_name\n",
    "\n",
    "        union all\n",
    "\n",
    "        select t.screen_name as screen_name1, m.mention_screen_name as screen_name2, count(*) as n_links,\n",
    "            'mention' as desc\n",
    "        from mentions as m\n",
    "        left join tweets as t\n",
    "        on t.tweet_id = m.tweet_id\n",
    "        where t.retweet_id is null\n",
    "        group by t.screen_name, m.mention_screen_name\n",
    "\"\"\")\n",
    "interaction_edge.registerTempTable('interaction_edge')\n",
    "interaction_edge.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283806"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_edge.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine all edges into one list\n",
    "all_edge = spark.sql(\"\"\"\n",
    "    select screen_name1, screen_name2, sum(n_links) as n_links\n",
    "    from\n",
    "        (select screen_name1, screen_name2, n_links\n",
    "        from tweet_rt_edge\n",
    "\n",
    "        union all\n",
    "\n",
    "        select screen_name1, screen_name2, n_links\n",
    "        from interaction_edge) sub\n",
    "    where not screen_name1 = 'raimu0003'\n",
    "    group by screen_name1, screen_name2\n",
    "\"\"\")\n",
    "# I had to delete the three tweets from 'raimu0003'\n",
    "# Their data caused an issue with the Gephi import, possibly because 'u0003' is a unicode character\n",
    "\n",
    "all_edge.registerTempTable('all_edge')\n",
    "all_edges = all_edge.rdd.map(lambda x: (x['screen_name1'], x['screen_name2'], {'weight':x['n_links']})).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884553"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save other user data into one list\n",
    "user_data = spark.sql(\"\"\"\n",
    "    select u.screen_name, u.name, u.description, u.followers_count,\n",
    "        case when c.total_rts is null then 0 else total_rts end as total_rts\n",
    "    from\n",
    "       (select screen_name, name, description, followers_count\n",
    "            from\n",
    "                (select screen_name, name, description, followers_count,\n",
    "                    row_number() over (partition by screen_name order by created_at desc) as n_repeats\n",
    "                from tweets) sub\n",
    "        where n_repeats = 1) as u\n",
    "    left join\n",
    "        (select screen_name, sum(retweet_count) as total_rts\n",
    "        from tweets\n",
    "        where retweet_id is null\n",
    "        group by screen_name) as c\n",
    "    on c.screen_name = u.screen_name\n",
    "\"\"\")\n",
    "user_data.registerTempTable('user_data')\n",
    "\n",
    "user_data_dict = {}\n",
    "for row in user_data.rdd.collect():\n",
    "    user_data_dict[row['screen_name']] = {'name':row['name'],\n",
    "                                        'description':row['description'],\n",
    "                                        'followers_count':row['followers_count'],\n",
    "                                        'total_rts':row['total_rts']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create graph object based on the above edges  \n",
    "### 2. Find giant component  \n",
    "### 3. Assign communities to each node in giant component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=nx.DiGraph()\n",
    "A.add_edges_from(all_edges)\n",
    "\n",
    "# Remove self RTs or mentions\n",
    "A.remove_edges_from(A.selfloop_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of largest connected components: [480331, 163, 89, 76, 73, 41, 39, 34, 30, 28, 26, 25, 25, 23, 23, 23, 22, 21, 20, 19]\n"
     ]
    }
   ],
   "source": [
    "# The giant component is vastly larger than all the other connected subgraphs in the network, so it's safe to\n",
    "# perform analysis only on the giant component.\n",
    "\n",
    "A_subgraphs = sorted(nx.connected_component_subgraphs(A.to_undirected()), key=len, reverse=True)\n",
    "print('Size of largest connected components:', [len(g) for g in A_subgraphs[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine which community each user is in\n",
    "partition = community.best_partition(A_subgraphs[0].to_undirected())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find largest strongly connected component (SCC)  \n",
    "### 2. Create graph from SCC  \n",
    "### 3. Calculate PageRank of all users in SCC  \n",
    "### 4. Save PageRank and Community data to SCC graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of largest strongly connected components: [14321, 12, 12, 11, 11, 9, 9, 9, 9, 8, 8, 8, 7, 7, 7, 7, 7, 7, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "# Find strongly connected components\n",
    "A_conn_comp = sorted(nx.strongly_connected_components(A),key=len, reverse=True)\n",
    "print('Size of largest strongly connected components:', [len(g) for g in A_conn_comp[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create graph of largest strongly connected component\n",
    "S = nx.subgraph(A, A_conn_comp[0])\n",
    "pr = nx.pagerank(S)\n",
    "\n",
    "# Save PageRank and Community Membership to the data dict for each node\n",
    "for n, d in S.nodes(data=True):\n",
    "    d['pagerank'] = pr[n]\n",
    "    d['community'] = partition[n]\n",
    "    d['name'] = user_data_dict[n]['name']\n",
    "    #d['description'] = user_data_dict[n]['description']\n",
    "    d['followers_count'] = user_data_dict[n]['followers_count']\n",
    "    d['total_rts'] = user_data_dict[n]['total_rts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export graph to Gephi\n",
    "nx.write_gexf(S, 'tweet_network_final.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save Community and PageRank data\n",
    "coms = sc.parallelize([(k, str(v)) for k, v in partition.items()]). \\\n",
    "    toDF(['screen_name', 'community'])\n",
    "coms.write.parquet('communities.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageranks = sc.parallelize([(k, str(v)) for k, v in pr.items()]). \\\n",
    "    toDF(['screen_name', 'pagerank'])\n",
    "pageranks.write.parquet('pageranks.parquet')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datasci3]",
   "language": "python",
   "name": "conda-env-datasci3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
