{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/conda/envs/datasci3/lib/python3.5/site-packages/PIL/Image.py:85: RuntimeWarning: The _imaging extension was built for another  version of Pillow or PIL\n",
      "  warnings.warn(str(v), RuntimeWarning)\n",
      "/home/ubuntu/conda/envs/datasci3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/conda/envs/datasci3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark, pickle\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, StopWordsRemover, NGram, RegexTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, re\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from sklearn import svm, grid_search, datasets\n",
    "from spark_sklearn import GridSearchCV\n",
    "\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv('labeled_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean tweet_data\n",
    "\n",
    "tweet_data.columns = ['garbage', 'tweet', 'existence', 'existence_conf']\n",
    "tweet_data.drop('garbage', axis=1, inplace=True)\n",
    "tweet_data.existence.replace(['Yes', 'No', 'Y', 'N'], ['yes', 'no', 'yes', 'no'], inplace=True)\n",
    "tweet_data.tweet = tweet_data.tweet.str.replace('https?://[^ ,]+', '[link]') #replace links with '[link]'\n",
    "tweet_data.existence.fillna('neutral', inplace=True)\n",
    "tweet_data = tweet_data.sort_values('existence_conf', ascending=False).drop_duplicates(subset=['tweet'])\n",
    "tweet_data = tweet_data[tweet_data.tweet != '[link]']   # One tweet consists only of a link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove 'neutral' tweets. Will build model using only 'accept' and 'deny' classes\n",
    "tweet_data = tweet_data[(tweet_data.existence != 'neutral')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3352, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "      <th>existence_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. [link]</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>New on [link] -- Heavy Snow Events:  Not a Contradiction to Global Warming Theory  -- [link]</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>Interesting... RT @Heritage The DC Blizzard:  More proof of Global Warming! [link]</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>@Adam4004 That's sort of beside the point. People are acting like a winter storm in the mid-Atlantic disproves global warming. It doesn't.</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>RT @time D.C. Snowstorm: How Global Warming Makes Blizzards Worse - TIME [link] #green #climate #smh</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           tweet  \\\n",
       "0     Global warming report urges governments to act|BRUSSELS, Belgium (AP) - The world faces increased hunger and .. [link]                       \n",
       "3146  New on [link] -- Heavy Snow Events:  Not a Contradiction to Global Warming Theory  -- [link]                                                 \n",
       "3109  Interesting... RT @Heritage The DC Blizzard:  More proof of Global Warming! [link]                                                           \n",
       "3111  @Adam4004 That's sort of beside the point. People are acting like a winter storm in the mid-Atlantic disproves global warming. It doesn't.   \n",
       "3124  RT @time D.C. Snowstorm: How Global Warming Makes Blizzards Worse - TIME [link] #green #climate #smh                                         \n",
       "\n",
       "     existence  existence_conf  \n",
       "0     yes       1.0             \n",
       "3146  yes       1.0             \n",
       "3109  yes       1.0             \n",
       "3111  yes       1.0             \n",
       "3124  yes       1.0             "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_data.shape)\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    0.706146\n",
       "no     0.293854\n",
       "Name: existence, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class balance\n",
    "tweet_data.existence.value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Spark dataframe and do train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+\n",
      "|               tweet|existence|existence_conf|\n",
      "+--------------------+---------+--------------+\n",
      "|Global warming re...|      yes|           1.0|\n",
      "|New on [link] -- ...|      yes|           1.0|\n",
      "|Interesting... RT...|      yes|           1.0|\n",
      "|@Adam4004 That's ...|      yes|           1.0|\n",
      "|RT @time D.C. Sno...|      yes|           1.0|\n",
      "|Actress Q'orianka...|      yes|           1.0|\n",
      "|@Mac80537 Climate...|      yes|           1.0|\n",
      "|Man! I wish there...|       no|           1.0|\n",
      "|RT @WWFUS Climate...|      yes|           1.0|\n",
      "|Fox News has Al G...|      yes|           1.0|\n",
      "|Climate Denial Cr...|      yes|           1.0|\n",
      "|FOX  \"You can't m...|       no|           1.0|\n",
      "|It's official: Th...|      yes|           1.0|\n",
      "|Damn global warmi...|      yes|           1.0|\n",
      "|Snowstorm: E Coas...|      yes|           1.0|\n",
      "|So much for globa...|       no|           1.0|\n",
      "|Article: If #clim...|      yes|           1.0|\n",
      "|THANKS! Its not g...|       no|           1.0|\n",
      "|RT @gemswinc: RT ...|      yes|           1.0|\n",
      "|RT @contessabrewe...|      yes|           1.0|\n",
      "+--------------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spark dataframe of tweets called 'tweets spark'\n",
    "\n",
    "tweet_sp = spark.createDataFrame(tweet_data)\n",
    "tweet_sp.persist()\n",
    "tweet_sp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2327\n",
      "1025\n"
     ]
    }
   ],
   "source": [
    "(train, test) = tweet_sp.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "train.persist()\n",
    "test.persist()\n",
    "\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes model\n",
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create regex tokenizer that is useful for Twitter data (preserves emoticons, hashtags, etc.)\n",
    "# I used code from here, with some modifications: https://github.com/adonoho/TweetTokenizers/blob/master/PottsTweetTokenizer.py\n",
    "\n",
    "# I got rid of ellipsis matcher\n",
    "# I modified words with apostrophes to not keep after the apostrophe\n",
    "# I got rid of the 'everything else that isn't whitespace' matcher\n",
    "# I added regex to capture words in quotes as separate items: r'(?:[\"\\'][a-zA-Z0-9/-]+[\"\\'])'\n",
    "pattern = r\"\"\"(?:\\[link\\])|(?:(?:\\+?[01][\\-\\s.]*)?(?:[\\(]?\\d{3}[\\-\\s.\\)]*)?\\d{3}[\\-\\s.]*\\d{4})|(?:(?<= )[<>]?[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]|[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\][\\-o\\*\\']?[:;=8][<>]?)|(<[^>]+>)|(?:@[\\w_]+)|(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|(?:[\"\\'][a-z0-9/-]+[\"\\'])|(?:[a-z][a-z\\-_]+[a-z])|(?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)|(?:[\\w_]+)\"\"\"\n",
    "\n",
    "word_re = re.compile(pattern, re.VERBOSE | re.I | re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|RT @contessabrewe...|\n",
      "|@OTOOLEFAN REAL s...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Regex\n",
    "df = spark.createDataFrame([\"RT @contessabrewer: @newsbusters That's not what I said. >:(  :D I said that snowstorms don't refute global warming. (RE: [link]\",\n",
    "                           \"@OTOOLEFAN REAL science. Not Algore's climate change 'science' hacks with their man-made global warming hoax. Gore=Palin\"],\n",
    "                           StringType())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=\"RT @contessabrewer: @newsbusters That's not what I said. >:(  :D I said that snowstorms don't refute global warming. (RE: [link]\", tokens=['rt', '@contessabrewer', '@newsbusters', 'that', 's', 'not', 'what', 'i', 'said', '>:(', ':d', 'i', 'said', 'that', 'snowstorms', 'don', 't', 'refute', 'global', 'warming', 're', '[link]']),\n",
       " Row(value=\"@OTOOLEFAN REAL science. Not Algore's climate change 'science' hacks with their man-made global warming hoax. Gore=Palin\", tokens=['@otoolefan', 'real', 'science', 'not', 'algore', 's', 'climate', 'change', \"'science'\", 'hacks', 'with', 'their', 'man-made', 'global', 'warming', 'hoax', 'gore', 'palin'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexTokenizer(inputCol=\"value\", outputCol=\"tokens\", gaps=False, pattern=word_re.pattern).transform(df).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this to check how labels are indexed by StringIndexer\n",
    "\n",
    "#StringIndexer(inputCol=\"existence\", outputCol=\"label\").fit(train).transform(train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number my labels\n",
    "# Label numbering goes in order of most frequent label, descending\n",
    "label_indxr = StringIndexer(inputCol=\"existence\", outputCol=\"label\")\n",
    "\n",
    "# Tokenize tweets\n",
    "tokenizer = RegexTokenizer(inputCol=\"tweet\", outputCol=\"tokens\", gaps=False, pattern=word_re.pattern)\n",
    "\n",
    "# Remove stopwords\n",
    "stp_rmv = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='new_tokens',\n",
    "                           stopWords=stopwords.words('english'))\n",
    "\n",
    "# Ngrams\n",
    "ngram = NGram(inputCol=stp_rmv.getOutputCol(), outputCol=\"ngrams\")\n",
    "\n",
    "# Count occurences of words\n",
    "cnvk = CountVectorizer(inputCol=ngram.getOutputCol(), outputCol='counts')\n",
    "\n",
    "# Train a NaiveBayes model\n",
    "nb = NaiveBayes(featuresCol=cnvk.getOutputCol(), modelType=\"multinomial\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[label_indxr, tokenizer, stp_rmv, ngram, cnvk, nb])\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                             metricName=\"accuracy\")\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='label',\n",
    "#                                         rawPredictionCol='prediction',\n",
    "#                                         metricName='areaUnderROC')\n",
    "\n",
    "# Program search params\n",
    "param_grid = (ParamGridBuilder() \n",
    "    .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]) \n",
    "    .addGrid(ngram.n, [1, 2, 3]) \\\n",
    "    .build())\n",
    "\n",
    "# Put pipeline together with param search\n",
    "cv_pipe = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "nb_model = cv_pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7356521881130746,\n",
       " 0.7560753512411365,\n",
       " 0.7511342511667729,\n",
       " 0.7800135610129255,\n",
       " 0.765709673123103,\n",
       " 0.7517374644013859,\n",
       " 0.7882529290815801,\n",
       " 0.7673312018901837,\n",
       " 0.7525598860436528,\n",
       " 0.7890630860255661,\n",
       " 0.7715647808409496,\n",
       " 0.7520081999961119,\n",
       " 0.7907668230432867,\n",
       " 0.7721222000017398,\n",
       " 0.7503277774958442,\n",
       " 0.7842581897828638,\n",
       " 0.7686791467505643,\n",
       " 0.7498791458224481]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average accuracy across grid search:\n",
    "nb_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 1,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.0},\n",
       "  0.7356521881130746),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 1,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.2},\n",
       "  0.7800135610129255),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 1,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.4},\n",
       "  0.7882529290815801),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 1,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.6},\n",
       "  0.7890630860255661),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 1,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.8},\n",
       "  0.7907668230432867),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 1,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0},\n",
       "  0.7842581897828638),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 2,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.0},\n",
       "  0.7560753512411365),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 2,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.2},\n",
       "  0.765709673123103),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 2,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.4},\n",
       "  0.7673312018901837),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 2,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.6},\n",
       "  0.7715647808409496),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 2,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.8},\n",
       "  0.7721222000017398),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 2,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0},\n",
       "  0.7686791467505643),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 3,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.0},\n",
       "  0.7511342511667729),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 3,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.2},\n",
       "  0.7517374644013859),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 3,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.4},\n",
       "  0.7525598860436528),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 3,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.6},\n",
       "  0.7520081999961119),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 3,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.8},\n",
       "  0.7503277774958442),\n",
       " ({Param(parent='NGram_45d1aef11812936053cc', name='n', doc='number of elements per n-gram (>=1)'): 3,\n",
       "   Param(parent='NaiveBayes_4ecbbd74374dfe252c01', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0},\n",
       "  0.7498791458224481)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show each model param alongside accuracy\n",
    "list(zip(param_grid, nb_model2.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.8117073170731708\n"
     ]
    }
   ],
   "source": [
    "# See accuracy on test set\n",
    "test_prediction = nb_model.transform(test)\n",
    "\n",
    "accuracy = evaluator.evaluate(test_prediction)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes recall: 0.8897849462365591\n",
      "No recall: 0.604982206405694 \n",
      "\n",
      "Yes precision: 0.8564036222509702\n",
      "No precision: 0.6746031746031746\n"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(test_prediction.select(\"prediction\", \"label\").rdd)\n",
    "\n",
    "print('Yes recall:', metrics.recall(0))\n",
    "print('No recall:', metrics.recall(1), '\\n')\n",
    "\n",
    "print('Yes precision:', metrics.precision(0))\n",
    "print('No precision:', metrics.precision(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88978495  0.11021505]\n",
      " [ 0.39501779  0.60498221]]\n"
     ]
    }
   ],
   "source": [
    "# Show normalized confusion matrix\n",
    "conf = metrics.confusionMatrix().toArray()\n",
    "conf_norm = conf/conf.sum(axis=1)[:, np.newaxis]\n",
    "print(conf_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save naive bayes model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_model.bestModel.save('./nb_model_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = PipelineModel.load('./nb_model_pipeline/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model\n",
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number my labels\n",
    "# Label numbering goes in order of most frequent label, descending\n",
    "label_indxr_svm = StringIndexer(inputCol=\"existence\", outputCol=\"label\")\n",
    "\n",
    "# Tokenize tweets\n",
    "tokenizer_svm = RegexTokenizer(inputCol=\"tweet\", outputCol=\"tokens\", gaps=False, pattern=word_re.pattern)\n",
    "\n",
    "# Remove stopwords\n",
    "stp_rmv_svm = StopWordsRemover(inputCol=tokenizer_svm.getOutputCol(), outputCol='new_tokens',\n",
    "                           stopWords=stopwords.words('english'))\n",
    "\n",
    "# Count occurences of words\n",
    "cnvk_svm = CountVectorizer(inputCol=stp_rmv_svm.getOutputCol(), outputCol='counts')\n",
    "\n",
    "# Pipeline\n",
    "pipeline_svm = Pipeline(stages=[label_indxr_svm, tokenizer_svm, stp_rmv_svm, cnvk_svm])\n",
    "\n",
    "# Create evaluator\n",
    "evaluator_svm = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                             metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model  \n",
    "## Pyspark SVM doesn't have rbf kernel, so I have to use spark_sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+-----+--------------------+--------------------+--------------------+\n",
      "|               tweet|existence|existence_conf|label|              tokens|          new_tokens|              counts|\n",
      "+--------------------+---------+--------------+-----+--------------------+--------------------+--------------------+\n",
      "|\"Even McCain supp...|       no|           1.0|  1.0|[even, mccain, su...|[even, mccain, su...|(6439,[1,2,50,102...|\n",
      "|\"Forests are grow...|      yes|           1.0|  0.0|[forests, are, gr...|[forests, growing...|(6439,[0,3,4,233,...|\n",
      "|\"How to fabricate...|       no|           1.0|  1.0|[how, to, fabrica...|[fabricate, clima...|(6439,[0,3,4,6,16...|\n",
      "|\"Kerry Graham Lie...|      yes|           1.0|  0.0|[kerry, graham, l...|[kerry, graham, l...|(6439,[0,1,2,3,37...|\n",
      "|\"Proof Of Global ...|      yes|           1.0|  0.0|[proof, of, globa...|[proof, global, w...|(6439,[0,1,2,145]...|\n",
      "|\"SCAM, SCAM, SCAM...|       no|           1.0|  1.0|[scam, scam, scam...|[scam, scam, scam...|(6439,[0,1,2,9,30...|\n",
      "|#ALMS Let me poli...|       no|           1.0|  1.0|[#alms, let, me, ...|[#alms, let, poli...|(6439,[1,2,195,43...|\n",
      "|#Africa2day Afric...|      yes|           1.0|  0.0|[#africa2day, afr...|[#africa2day, afr...|(6439,[0,3,4,35,1...|\n",
      "|#California's Riv...|      yes|           1.0|  0.0|[#california's, r...|[#california's, r...|(6439,[0,4,10,87,...|\n",
      "|#Climate change =...|      yes|           1.0|  0.0|[#climate, change...|[#climate, change...|(6439,[0,4,10,56,...|\n",
      "|#Climate change, ...|      yes|           1.0|  0.0|[#climate, change...|[#climate, change...|(6439,[4,10,67,21...|\n",
      "|#FF @Revkin for a...|      yes|           1.0|  0.0|[#ff, @revkin, fo...|[#ff, @revkin, re...|(6439,[4,10,15,67...|\n",
      "|#Globalwarming #H...|       no|           1.0|  1.0|[#globalwarming, ...|[#globalwarming, ...|(6439,[0,4,9,10,3...|\n",
      "|#enviroment: Glob...|      yes|           1.0|  0.0|[#enviroment, glo...|[#enviroment, glo...|(6439,[0,1,2,223,...|\n",
      "|#news Dissecting ...|      yes|           1.0|  0.0|[#news, dissectin...|[#news, dissectin...|(6439,[0,1,2,28,1...|\n",
      "|#news How to talk...|      yes|           1.0|  0.0|[#news, how, to, ...|[#news, talk, fri...|(6439,[0,3,4,216,...|\n",
      "|#prediction Time ...|       no|           1.0|  1.0|[#prediction, tim...|[#prediction, tim...|(6439,[1,2,32,171...|\n",
      "|(\"Censored News\")...|      yes|           1.0|  0.0|[censored, news, ...|[censored, news, ...|(6439,[0,3,4,11,1...|\n",
      "|. Global Warming:...|      yes|           1.0|  0.0|[global, warming,...|[global, warming,...|(6439,[0,1,2,60,1...|\n",
      "|10 Sustainable Su...|      yes|           1.0|  0.0|[10, sustainable,...|[10, sustainable,...|(6439,[0,1,3,4,71...|\n",
      "+--------------------+---------+--------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorize my data\n",
    "countvec_data_svm = pipeline_svm.fit(train).transform(train)\n",
    "countvec_data_svm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split up the sparse matrix that Spark gives me and create a standard sklearn X matrix\n",
    "countvec_data_svm_x = pd.DataFrame(countvec_data_svm.select('counts').rdd.map(\n",
    "                        lambda x: x['counts'].toArray()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create standard sklearn Y vector\n",
    "countvec_data_svm_y = countvec_data_svm.select('existence').toPandas().existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}, {'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       sc=<pyspark.context.SparkContext object at 0x7fa8efe04eb8>,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "parameters = [{'kernel': ['rbf'], 'gamma': [1,1e-1,1e-2,1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "svr = svm.SVC()\n",
    "clf = GridSearchCV(sc, svr, parameters)\n",
    "clf.fit(countvec_data_svm_x, countvec_data_svm_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model based on CV accuracy, and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666523420713365"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV accuracy:\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run test data through pipeline\n",
    "test_svm = pipeline_svm.fit(train).transform(test)\n",
    "test_svm_x = pd.DataFrame(test_svm.select('counts').rdd.map(\n",
    "                        lambda x: x['counts'].toArray()).collect())\n",
    "test_svm_y = test_svm.select('existence').toPandas().existence.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "svm_predicts = clf.predict(test_svm_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>existence</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  existence predict\n",
       "0  no        no    \n",
       "1  yes       yes   \n",
       "2  yes       no    \n",
       "3  yes       no    \n",
       "4  yes       yes   "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare predictions and ground truth\n",
    "test_svm_y['predict'] = svm_predicts\n",
    "test_svm_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "existence  predict\n",
       "no         yes        0.679715\n",
       "           no         0.320285\n",
       "yes        yes        0.862903\n",
       "           no         0.137097\n",
       "Name: predict, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall for each class\n",
    "test_svm_y.groupby('existence').predict.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict  existence\n",
       "no       yes          0.531250\n",
       "         no           0.468750\n",
       "yes      yes          0.770708\n",
       "         no           0.229292\n",
       "Name: existence, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision for each class\n",
    "test_svm_y.groupby('predict').existence.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SVM performs worse than Naive Bayes here, so I won't keep it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:datasci3]",
   "language": "python",
   "name": "conda-env-datasci3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
